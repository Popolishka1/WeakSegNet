{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this second experiment, I look into buiding the minimum viable product for another use case: City segmentation for autonomous driving. One model is trained using Weak supervision while the other will use a fully supervised approach. Here code will be available to call the saved models and do some more advanced comparison between the two:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pb: Downloader les datasets ca demande beaucoup d'espace de storage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weakly supervised approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torchvision import transforms, models, datasets\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "# Define the number of semantic classes.\n",
    "# (Cityscapes is usually mapped to 19 classes; here we assume that the segmentation masks have already been mapped accordingly.)\n",
    "num_classes = 19\n",
    "\n",
    "# Custom dataset wrapper for weak supervision.\n",
    "# It uses the Cityscapes 'fine' segmentation masks to derive multi-label targets.\n",
    "class CityscapesWeak(Dataset):\n",
    "    def __init__(self, root, split=\"train\", mode=\"fine\", target_type=\"semantic\", transform=None):\n",
    "        self.cityscapes = datasets.Cityscapes(root=root, split=split, mode=mode, target_type=target_type,\n",
    "                                               transform=transform)\n",
    "        self.transform = transform\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.cityscapes)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image, mask = self.cityscapes[idx]\n",
    "        # Convert mask to a NumPy array.\n",
    "        mask_np = np.array(mask)\n",
    "        # Create a multi-label vector: for each class, mark 1 if it appears in the mask.\n",
    "        labels = np.zeros(self.num_classes, dtype=np.float32)\n",
    "        unique_labels = np.unique(mask_np)\n",
    "        # Remove the ignore label (often 255) if present.\n",
    "        unique_labels = unique_labels[unique_labels != 255]\n",
    "        # For demonstration, assume unique_labels are in the range [0, num_classes-1].\n",
    "        for lab in unique_labels:\n",
    "            if lab < self.num_classes:\n",
    "                labels[int(lab)] = 1.0\n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image)\n",
    "        return image, torch.from_numpy(labels)\n",
    "\n",
    "# Define image transform (resize and normalize).\n",
    "transform_img = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Create the weakly supervised dataset and DataLoader.\n",
    "cityscapes_weak = CityscapesWeak(root='./data/cityscapes', split=\"train\", mode=\"fine\",\n",
    "                                  target_type=\"semantic\", transform=transform_img)\n",
    "batch_size = 8\n",
    "weak_loader = DataLoader(cityscapes_weak, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "\n",
    "# Define a ResNet50 classification model.\n",
    "model_ws = models.resnet50(pretrained=False)\n",
    "# Replace the final fully connected layer to output logits for num_classes.\n",
    "model_ws.fc = nn.Linear(model_ws.fc.in_features, num_classes)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_ws = model_ws.to(device)\n",
    "\n",
    "# Use BCEWithLogitsLoss for multi-label classification.\n",
    "criterion_ws = nn.BCEWithLogitsLoss()\n",
    "optimizer_ws = optim.Adam(model_ws.parameters(), lr=1e-3)\n",
    "\n",
    "# Training loop for weak supervision.\n",
    "num_epochs = 5  # Adjust as needed.\n",
    "model_ws.train()\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    for images, labels in weak_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        optimizer_ws.zero_grad()\n",
    "        outputs = model_ws(images)  # outputs: (B, num_classes)\n",
    "        loss = criterion_ws(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer_ws.step()\n",
    "        running_loss += loss.item()\n",
    "    print(f\"Weak Sup Epoch [{epoch+1}/{num_epochs}] - Loss: {running_loss/len(weak_loader):.4f}\")\n",
    "\n",
    "# Save the trained weakly supervised model.\n",
    "torch.save(model_ws.state_dict(), \"cityscapes_weak_classification.pth\")\n",
    "print(\"Weakly supervised model trained and saved.\")\n",
    "\n",
    "# --- Generating CAM from the Trained Weakly Supervised Model ---\n",
    "\n",
    "# Global variable to store feature maps.\n",
    "feature_maps_ws = None\n",
    "def hook_feature_ws(module, input, output):\n",
    "    global feature_maps_ws\n",
    "    feature_maps_ws = output.detach()\n",
    "\n",
    "# Register the hook on layer4 of ResNet50.\n",
    "model_ws.layer4.register_forward_hook(hook_feature_ws)\n",
    "\n",
    "def generate_cam_ws(model, img_tensor, target_class):\n",
    "    global feature_maps_ws\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        _ = model(img_tensor.to(device))\n",
    "    # Obtain the final FC layer weights.\n",
    "    fc_weights = model.fc.weight.data.cpu().numpy()\n",
    "    fmap = feature_maps_ws.cpu().numpy()[0]  # shape: (C, H, W)\n",
    "    cam = np.zeros(fmap.shape[1:], dtype=np.float32)\n",
    "    for i, w in enumerate(fc_weights[target_class]):\n",
    "        cam += w * fmap[i, :, :]\n",
    "    cam = np.maximum(cam, 0)\n",
    "    # Resize CAM to input image dimensions.\n",
    "    import cv2  # Ensure opencv-python is installed.\n",
    "    cam = cv2.resize(cam, (img_tensor.size(3), img_tensor.size(2)))\n",
    "    cam -= np.min(cam)\n",
    "    if np.max(cam) != 0:\n",
    "        cam /= np.max(cam)\n",
    "    return cam\n",
    "\n",
    "# Visualize CAM for a sample image.\n",
    "sample_img, sample_labels = cityscapes_weak[0]\n",
    "sample_tensor = sample_img.unsqueeze(0)\n",
    "# For demonstration, pick a target class (e.g., class 0). In practice, choose based on predicted output.\n",
    "target_class = 0\n",
    "cam_output = generate_cam_ws(model_ws, sample_tensor, target_class)\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.subplot(1,2,1)\n",
    "# Display the sample image (convert tensor to numpy image).\n",
    "img_disp = sample_img.permute(1,2,0).cpu().numpy()\n",
    "plt.imshow(img_disp)\n",
    "plt.title(\"Sample Image\")\n",
    "plt.axis(\"off\")\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(cam_output, cmap='jet')\n",
    "plt.title(\"CAM for Class 0\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fully supervised approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torchvision import transforms, models, datasets\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "# Define image transformation for Cityscapes images.\n",
    "transform_img = transforms.Compose([\n",
    "    transforms.Resize((512, 256)),  # Cityscapes images are high resolution; adjust as needed.\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Define a target transformation for segmentation masks.\n",
    "def mask_transform(mask):\n",
    "    # Resize the mask using nearest neighbor to preserve label integrity.\n",
    "    mask = mask.resize((512, 256), resample=Image.NEAREST)\n",
    "    mask_np = np.array(mask).astype(np.int64)\n",
    "    # Note: In the Cityscapes fine annotations, labels may need remapping to 19 classes.\n",
    "    # Here we assume the masks are already mapped appropriately (values in 0..18).\n",
    "    return torch.from_numpy(mask_np)\n",
    "\n",
    "# Create the Cityscapes dataset for semantic segmentation.\n",
    "cityscapes_dataset = datasets.Cityscapes(root='./data/cityscapes', split='train', mode='fine',\n",
    "                                          target_type='semantic', transform=transform_img,\n",
    "                                          target_transform=mask_transform)\n",
    "batch_size = 4\n",
    "cityscapes_loader = DataLoader(cityscapes_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "\n",
    "# Define the DeepLabV3 segmentation model.\n",
    "model_fs = models.segmentation.deeplabv3_resnet50(pretrained=True)\n",
    "# Replace the classifier head to output 19 classes.\n",
    "model_fs.classifier = models.segmentation.deeplabv3.DeepLabHead(2048, 19)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_fs = model_fs.to(device)\n",
    "\n",
    "# Define the loss and optimizer.\n",
    "criterion_fs = nn.CrossEntropyLoss(ignore_index=255)  # 255 is often used as the ignore label in Cityscapes.\n",
    "optimizer_fs = optim.Adam(model_fs.parameters(), lr=1e-4)\n",
    "num_epochs = 10  # Adjust epochs as needed.\n",
    "\n",
    "def train_one_epoch_fs(model, dataloader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for images, masks in dataloader:\n",
    "        images = images.to(device)\n",
    "        masks = masks.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)['out']  # DeepLabV3 returns a dict; 'out' contains the segmentation logits.\n",
    "        loss = criterion(outputs, masks)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    return running_loss / len(dataloader)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_loss = train_one_epoch_fs(model_fs, cityscapes_loader, optimizer_fs, criterion_fs, device)\n",
    "    print(f\"Fully Sup Epoch [{epoch+1}/{num_epochs}] - Loss: {epoch_loss:.4f}\")\n",
    "\n",
    "# Save the fully supervised model.\n",
    "torch.save(model_fs.state_dict(), \"cityscapes_fully_supervised.pth\")\n",
    "print(\"Fully supervised model trained and saved.\")\n",
    "\n",
    "# --- Visualization of a Segmentation Prediction ---\n",
    "def visualize_fs_prediction(model, dataset, index=0):\n",
    "    model.eval()\n",
    "    image, true_mask = dataset[index]\n",
    "    image_batch = image.unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        output = model(image_batch)['out']\n",
    "    pred_mask = torch.argmax(output.squeeze(), dim=0).cpu().numpy()\n",
    "    # Convert image back to displayable format (undo normalization).\n",
    "    inv_norm = transforms.Normalize(\n",
    "        mean=[-0.485/0.229, -0.456/0.224, -0.406/0.225],\n",
    "        std=[1/0.229, 1/0.224, 1/0.225]\n",
    "    )\n",
    "    image_disp = image.permute(1,2,0).cpu().numpy()\n",
    "    fig, axs = plt.subplots(1, 3, figsize=(15,5))\n",
    "    axs[0].imshow(image_disp)\n",
    "    axs[0].set_title(\"Input Image\")\n",
    "    axs[0].axis(\"off\")\n",
    "    axs[1].imshow(true_mask, cmap='gray')\n",
    "    axs[1].set_title(\"Ground Truth Mask\")\n",
    "    axs[1].axis(\"off\")\n",
    "    axs[2].imshow(pred_mask, cmap='gray')\n",
    "    axs[2].set_title(\"Predicted Mask\")\n",
    "    axs[2].axis(\"off\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Visualize a prediction on a sample image.\n",
    "visualize_fs_prediction(model_fs, cityscapes_dataset, index=0)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
