{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this Jupyter notebook, I train two models, one using WSSS and the other with a fully supervised framework. \n",
    "Issue: we would need to use one of the three pip installs - pip install opencv\n",
    "\n",
    "Look into data augmentation? See if that changes anything?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training CAM model from scratch (WSSS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 792M/792M [00:43<00:00, 18.1MB/s] \n",
      "100%|██████████| 19.2M/19.2M [00:01<00:00, 17.6MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of classes: 37\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/paulchainieux/miniconda3/lib/python3.12/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/Users/paulchainieux/miniconda3/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10] - Loss: 3.5969\n",
      "Epoch [2/10] - Loss: 3.2959\n",
      "Epoch [3/10] - Loss: 3.0917\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torchvision import transforms, datasets, models\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import cv2\n",
    "\n",
    "# 1. Setup and Data Preparation\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Oxford-IIIT Pet dataset with image-level (category) labels.\n",
    "dataset = datasets.OxfordIIITPet(root='./data', split='trainval', target_types='category', transform=transform, download=True)\n",
    "\n",
    "# Number of classes (labels are provided as numbers starting from 0)\n",
    "num_classes = len(dataset.classes)\n",
    "print(f\"Number of classes: {num_classes}\")\n",
    "\n",
    "# Create a DataLoader\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "\n",
    "# 2. Model Definition \n",
    "# Initialise ResNet18 without pre-trained weights\n",
    "model = models.resnet18(pretrained=False)\n",
    "# Replace the final fully connected layer to match the number of classes\n",
    "model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
    "\n",
    "# Move model to device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "# 3. Loss and Optimiser\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# 4. Training Loop\n",
    "num_epochs = 10\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    for i, (inputs, labels) in enumerate(train_loader):\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "    avg_loss = running_loss / len(train_loader)\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}] - Loss: {avg_loss:.4f}\")\n",
    "\n",
    "# Save the trained mode\n",
    "torch.save(model.state_dict(), \"pet_cam_model.pth\")\n",
    "print(\"Training complete and model saved.\")\n",
    "\n",
    "# 5. Generate CAM from the Trained Model\n",
    "feature_maps = None\n",
    "\n",
    "def hook_feature(module, input, output):\n",
    "    global feature_maps\n",
    "    feature_maps = output.detach()\n",
    "\n",
    "# Register hook on layer4\n",
    "model.layer4.register_forward_hook(hook_feature)\n",
    "\n",
    "def generate_cam(model, img_tensor, target_class=None):\n",
    "    global feature_maps\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        output = model(img_tensor.to(device))\n",
    "    if target_class is None:\n",
    "        target_class = output.argmax(dim=1).item()\n",
    "    \n",
    "    # Get the weights of the final fully connected layer for the target class.\n",
    "    fc_weights = model.fc.weight.data.cpu().numpy() \n",
    "    \n",
    "    # Get the feature maps captured by the hook: shape (1, C, H, W)\n",
    "    fmap = feature_maps.cpu().numpy()[0]\n",
    "    \n",
    "    # Compute the CAM as the weighted sum of feature maps.\n",
    "    cam = np.zeros(fmap.shape[1:], dtype=np.float32)\n",
    "    for i, w in enumerate(fc_weights[target_class]):\n",
    "        cam += w * fmap[i, :, :]\n",
    "    \n",
    "    # Normalise the CAM\n",
    "    cam = np.maximum(cam, 0)\n",
    "    cam = cv2.resize(cam, (img_tensor.size(3), img_tensor.size(2)))\n",
    "    cam -= np.min(cam)\n",
    "    if np.max(cam) != 0:\n",
    "        cam /= np.max(cam)\n",
    "    return cam, target_class\n",
    "\n",
    "# 6. Visualise CAM on a Sample Image\n",
    "def visualize_cam(original_img, cam, target_class):\n",
    "    plt.figure(figsize=(10,5))\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.imshow(original_img)\n",
    "    plt.title(\"Original Image\")\n",
    "    plt.axis(\"off\")\n",
    "    \n",
    "    plt.subplot(1,2,2)\n",
    "    plt.imshow(original_img)\n",
    "    plt.imshow(cam, cmap='jet', alpha=0.5)  # overlay CAM\n",
    "    plt.title(f\"CAM Overlay (Class: {target_class})\")\n",
    "    plt.axis(\"off\")\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "# Pick a sample image from the dataset\n",
    "sample_img, _ = dataset[0]\n",
    "# We need the original PIL image for visualisation, so reload it without normalisation:\n",
    "original_pil = Image.open(os.path.join(dataset._imgs[0])).convert('RGB') if hasattr(dataset, '_imgs') else dataset[0][0].permute(1,2,0).numpy()\n",
    "sample_tensor = sample_img.unsqueeze(0)\n",
    "\n",
    "cam, predicted_class = generate_cam(model, sample_tensor)\n",
    "print(f\"Predicted Class for Sample Image: {predicted_class}\")\n",
    "\n",
    "# Convert tensor to PIL image for visualisation (undo normalisation)\n",
    "def tensor_to_pil(tensor):\n",
    "    inv_normalize = transforms.Normalize(\n",
    "        mean=[-0.485/0.229, -0.456/0.224, -0.406/0.225],\n",
    "        std=[1/0.229, 1/0.224, 1/0.225]\n",
    "    )\n",
    "    tensor = inv_normalize(tensor.squeeze(0)).clamp(0, 1)\n",
    "    np_img = tensor.cpu().permute(1,2,0).numpy()\n",
    "    return Image.fromarray((np_img * 255).astype(np.uint8))\n",
    "\n",
    "original_img = tensor_to_pil(sample_tensor)\n",
    "\n",
    "visualize_cam(original_img, cam, predicted_class)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training CAM model from scratch (fully supervised)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torchvision import transforms, datasets, models\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "# 1. Data Preparation\n",
    "class SegmentationMaskTransform:\n",
    "    def __init__(self, size=(224, 224)):\n",
    "        self.size = size\n",
    "\n",
    "    def __call__(self, mask):\n",
    "        mask = mask.resize(self.size, resample=Image.NEAREST)\n",
    "        mask_np = np.array(mask).astype(np.int64)\n",
    "        # The masks in this dataset have values in {1,2,3}. Subtract 1 to get {0,1,2}.\n",
    "        mask_np = mask_np - 1\n",
    "        return torch.from_numpy(mask_np)\n",
    "\n",
    "# Define the transform for images.\n",
    "image_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Define a combined transform for the dataset.\n",
    "class OxfordPetSegmentationDataset(datasets.OxfordIIITPet):\n",
    "    def __init__(self, root, split='trainval', transform=None, target_transform=None, download=False):\n",
    "        super().__init__(root, split=split, target_types=\"segmentation\", transform=transform,\n",
    "                         target_transform=target_transform, download=download)\n",
    "\n",
    "# Create training dataset and loader.\n",
    "dataset = OxfordPetSegmentationDataset(\n",
    "    root='./data',\n",
    "    split='trainval',\n",
    "    transform=image_transform,\n",
    "    target_transform=SegmentationMaskTransform(size=(224, 224)),\n",
    "    download=True\n",
    ")\n",
    "\n",
    "batch_size = 8\n",
    "train_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "\n",
    "# 2. Model Definition (DeepLabV3)\n",
    "model = models.segmentation.deeplabv3_resnet50(pretrained=True)\n",
    "# The classifier head is a DeepLabHead: we replace it.\n",
    "model.classifier = models.segmentation.deeplabv3.DeepLabHead(2048, 3)\n",
    "\n",
    "# Move model to device (GPU if available).\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "# 3. Loss and Optimiser\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "# 4. Training Loop\n",
    "num_epochs = 10 \n",
    "\n",
    "def train_one_epoch(model, dataloader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for images, masks in dataloader:\n",
    "        images = images.to(device)\n",
    "        # masks should be LongTensor with shape (B, H, W)\n",
    "        masks = masks.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        # DeepLabV3 returns a dict; 'out' is the segmentation prediction.\n",
    "        outputs = model(images)['out']  \n",
    "        # outputs shape: (B, 3, H, W)\n",
    "        loss = criterion(outputs, masks)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    return running_loss / len(dataloader)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_loss = train_one_epoch(model, train_loader, optimizer, criterion, device)\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}] - Loss: {epoch_loss:.4f}\")\n",
    "\n",
    "# Save the fully supervised model\n",
    "torch.save(model.state_dict(), \"fully_supervised_pet_segmentation.pth\")\n",
    "print(\"Fully supervised model training complete and saved.\")\n",
    "\n",
    "# 5. Visualisation of Predictions\n",
    "def visualize_prediction(model, dataset, index=0):\n",
    "    model.eval()\n",
    "    image, true_mask = dataset[index]\n",
    "    image_batch = image.unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        output = model(image_batch)['out']\n",
    "    # Get the predicted segmentation mask (choose the class with highest probability per pixel)\n",
    "    pred_mask = torch.argmax(output.squeeze(), dim=0).cpu().numpy()\n",
    "    \n",
    "    # Convert image back to PIL for display (undo normalisation)\n",
    "    inv_normalize = transforms.Normalize(\n",
    "        mean=[-0.485/0.229, -0.456/0.224, -0.406/0.225],\n",
    "        std=[1/0.229, 1/0.224, 1/0.225]\n",
    "    )\n",
    "    image_disp = inv_normalize(image).clamp(0,1).permute(1,2,0).cpu().numpy()\n",
    "    \n",
    "    fig, axs = plt.subplots(1, 3, figsize=(15, 5))\n",
    "    axs[0].imshow(image_disp)\n",
    "    axs[0].set_title(\"Input Image\")\n",
    "    axs[0].axis(\"off\")\n",
    "    \n",
    "    axs[1].imshow(true_mask, cmap='gray')\n",
    "    axs[1].set_title(\"Ground Truth Mask\")\n",
    "    axs[1].axis(\"off\")\n",
    "    \n",
    "    axs[2].imshow(pred_mask, cmap='gray')\n",
    "    axs[2].set_title(\"Predicted Mask\")\n",
    "    axs[2].axis(\"off\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Visualise a prediction on a sample image from the dataset.\n",
    "visualize_prediction(model, dataset, index=0)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
